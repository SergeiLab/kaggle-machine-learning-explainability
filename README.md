# Machine Learning Explainability â€” Kaggle Course Solutions

This repository contains my completed exercises, experiments, and notes from the **[Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability)** course by Kaggle Learn.  

The course dives into methods that help you understand and interpret machine learning models â€” a key skill for ensuring transparency, debugging, and trust in predictions.

---

## ðŸ“˜ Course Overview

The **Machine Learning Explainability** course covers techniques and tools to interpret predictive models, especially complex ones like ensemble or black-box models.

### Topics covered:
1. **Global versus Local Explanations**
2. **Permutations and Feature Importance**
3. **SHAP (SHapley Additive exPlanations)**
4. **ICE (Individual Conditional Expectation)**
5. **Partial Dependence Plots (PDPs)**
6. **Interpretability pitfalls and best practices**

---

## ðŸ’¡ What I Learned

- How to distinguish between **global** and **local interpretability**.  
- Applying **permutation feature importance** to assess model reliance on features.  
- Using **SHAP values** to explain individual predictions and feature interactions.  
- Creating **ICE plots** and **partial dependence plots** (PDPs) to probe feature effects.  
- Recognizing pitfalls and biases in interpretation (e.g. correlated features, interaction effects).  
- Integrating explainability workflows into model evaluation and communication.

---

## ðŸ§  Tools and Libraries Used

- **Python 3**  
- **Pandas**, **NumPy**  
- **Scikit-learn**  
- **SHAP**  
- **Matplotlib**, **Seaborn**  
- **Plotly**, **PDPbox** (if used in the exercises)

---

## ðŸ“‚ Repository Structure
â”‚ â”œâ”€â”€ 01_global_vs_local.ipynb
â”‚ â”œâ”€â”€ 02_permutation_importance.ipynb
â”‚ â”œâ”€â”€ 03_shap_values.ipynb
â”‚ â”œâ”€â”€ 04_ice_plots.ipynb
â”‚ â”œâ”€â”€ 05_partial_dependence.ipynb
â”‚ â”œâ”€â”€ 06_interpretability_pitfalls.ipynb
â”‚
â””â”€â”€ README.md
